{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JupyterImport import *\n",
    "from src.DataStandardize.ExpSpecific.Sep20 import Fixes\n",
    "from src.DatObject.Attributes import SquareEntropy as SE, Transition as T, Entropy as E, DatAttribute as DA, Logs as L\n",
    "root_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (15 of 15) |########################| Elapsed Time: 0:00:02 Time:  0:00:02\n"
     ]
    }
   ],
   "source": [
    "# dats = get_dats(range(363, 366+1))  # Test Entropy measurements with no Channel Bias, first two 0mT second two -100mT field 100mK\n",
    "# dats = get_dats(range(368, 373+1))  # Measurements a bit noisy, not quite a full set 100mK, -100mT\n",
    "# dats = get_dats(range(374, 377+1))  # Accidentally measured 1-> 2 transition 100mK, -100mT\n",
    "# dats = get_dats(range(378, 384+1))  # 0 -> 1 transition with -0.035 ratio (which turned out to be a bit too high) -100mT field, 100mK\n",
    "# dats = get_dats(range(385, 389+1))  # 0 -> 1 transition with -0.028 ratio (best from above) -100mT, 100mK\n",
    "# dats = get_dats(range(488, 517+1))  # Along 0->1 transition starting from gamma broadened, -100mT, 100mK, gamma broadened scans are wider -- Not enough heating\n",
    "# dats = get_dats(range(518, 547+1))  # Same as above with 5x heating -- Turned out that HQPCs were not set despite the recorded DAC values.\n",
    "dats = get_dats(range(581, 595+1))  # Same as above but with working HQPCs and Heating bias set back to +-300mV (~25%)\n",
    "\n",
    "save_graphs=False\n",
    "filter_on = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dat581: LCT=-459.9\n",
      "Dat582: LCT=-449.83\n",
      "Dat583: LCT=-439.76\n",
      "Dat584: LCT=-429.99\n",
      "Dat585: LCT=-419.92\n",
      "Dat586: LCT=-409.85\n",
      "Dat587: LCT=-399.78\n",
      "Dat588: LCT=-389.71\n",
      "Dat589: LCT=-379.94\n",
      "Dat590: LCT=-369.87\n",
      "Dat591: LCT=-364.99\n",
      "Dat592: LCT=-359.8\n",
      "Dat593: LCT=-354.92\n",
      "Dat594: LCT=-349.73\n",
      "Dat595: LCT=-344.85\n"
     ]
    }
   ],
   "source": [
    "for dat in dats:\n",
    "    print(f\"Dat{dat.datnum}: LCT={dat.Logs.fds['LCT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (15 of 15) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "for dat in progressbar(dats, redirect_stdout=False):\n",
    "        if 'LS625 Magnet Supply' in dat.Logs.full_sweeplogs.keys():\n",
    "            Fixes._add_magy(dat)  # Adds temporary magy field to Other.magy\n",
    "        else:\n",
    "            dat.Other.magy = L.MAGs(name='magy', field=0, rate='50')\n",
    "        dat.SquareEntropy.Processed.plot_info.show = SE.ShowPlots(info=True, raw=True, averaged=True, entropy=True)\n",
    "        dat.SquareEntropy.ShowPlots.integrated = True\n",
    "dats = [dat for dat in dats if dat.SquareEntropy.Processed.outputs.entropy_fit is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Square Entropy stuff for Dat595\n"
     ]
    }
   ],
   "source": [
    "for dat in dats:\n",
    "    if dat.Logs.fds['LCT'] > -345:  # Force blind averaging for super gamma broadened dats\n",
    "        print(f'Resetting Square Entropy stuff for Dat{dat.datnum}')\n",
    "        fi = DA.FitInfo()\n",
    "        out = dat.SquareEntropy.Processed.outputs\n",
    "        inp = dat.SquareEntropy.Processed.inputs\n",
    "        out.averaged = np.mean(out.cycled, axis=0)\n",
    "        x = np.linspace(out.x[0], out.x[-1], out.averaged.shape[-1])\n",
    "        \n",
    "        fit = E.entropy_fits(x, out.averaged)[0]\n",
    "        fi.init_from_fit(fit)\n",
    "        out.entropy_fit = fi\n",
    "        dat.SquareEntropy.Processed.outputs = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 15) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'amp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c2fe390cdd73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mallowed_amps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mamp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mallowed_amps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mallowed_dTs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdT\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mallowed_dTs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_amps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mdat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSquareEntropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'amp' is not defined"
     ]
    }
   ],
   "source": [
    "allowed_amps = (0.5, 1.2)\n",
    "default_amp = 1\n",
    "allowed_dTs = (4.3, 6.3)\n",
    "default_dT = 5.3\n",
    "\n",
    "xs, trans_datas, entropy_datas, integrated_datas, ids, trans_titles, entropy_titles, integrated_titles, tcs, ths, dTs = list(), list(), list(), list(), list(), list(), list(), list(), list(), list(), list()\n",
    "line = lm.models.LinearModel()\n",
    "for dat in progressbar(dats):\n",
    "\n",
    "    x = dat.SquareEntropy.Processed.outputs.x\n",
    "    \n",
    "    ids.append(f'{dat.datnum}')\n",
    "    \n",
    "    dat_tcs, dat_amps = list(), list()\n",
    "    for data in dat.SquareEntropy.Processed.outputs.averaged[0::2]:\n",
    "        fit = T.transition_fits(x, data, func=T.i_sense)[0]\n",
    "        dat_tcs.append(fit.best_values['theta'])\n",
    "        dat_amps.append(fit.best_values['amp'])\n",
    "    dat_ths = list()\n",
    "    for data in dat.SquareEntropy.Processed.outputs.averaged[1::2]:\n",
    "        fit = T.transition_fits(x, data, func=T.i_sense)[0]\n",
    "        dat_ths.append(fit.best_values['theta'])\n",
    "    tc = np.nanmean(dat_tcs)\n",
    "    th = np.nanmean(dat_ths)\n",
    "    dT = th-tc\n",
    "    tcs.append(tc)\n",
    "    ths.append(th)\n",
    "    dTs.append(dT)\n",
    "    \n",
    "    \n",
    "    if allowed_amps[0] < amp < allowed_amps[1] and allowed_dTs[0] < dT < allowed_dTs[1] and True:\n",
    "        amp = np.average(dat_amps)\n",
    "        dat.SquareEntropy.Processed.inputs.dT = dT\n",
    "        dat.SquareEntropy.Processed.inputs.transition_amplitude = amp\n",
    "        dat.SquareEntropy.process()\n",
    "        integrated_data = dat.SquareEntropy.Processed.outputs.integrated_entropy\n",
    "        indexs = CU.get_data_index(x, [-1500, -1000])\n",
    "        line_fit = line.fit(integrated_data[indexs[0]:indexs[1]], x=x[indexs[0]:indexs[1]], nan_policy='omit')\n",
    "        integrated_data = integrated_data-line_fit.eval(x=x)\n",
    "    else:  # Manually calculate integrated (processing will mess up stuff)\n",
    "        out = dat.SquareEntropy.Processed.outputs\n",
    "        dx = np.mean(np.diff(x))\n",
    "        dt = default_dT\n",
    "        amp = amp if allowed_amps[0] < amp < allowed_amps[1] else default_amp\n",
    "        out.entropy_signal = SE.entropy_signal(out.averaged)\n",
    "        sf = SE.scaling(dt=dt, amplitude=amp, dx=dx)\n",
    "        int_info = SE.IntegratedInfo(dT=dt, amp=amp, dx=dx)\n",
    "        out.integrated_entropy = SE.integrate_entropy(out.entropy_signal, int_info.sf)\n",
    "        int_info.dS = out.integrated_entropy[-1]\n",
    "        out.integrated_info = int_info\n",
    "        integrated_data = out.integrated_entropy\n",
    "#         indexs = CU.get_data_index(x, [-4500, -2500])\n",
    "#         line_fit = line.fit(integrated_data[indexs[0]:indexs[1]], x=x[indexs[0]:indexs[1]], nan_policy='omit')\n",
    "#         integrated_data = integrated_data-line_fit.eval(x=x)\n",
    "    \n",
    "    \n",
    "    e_signal = dat.SquareEntropy.Processed.outputs.entropy_signal\n",
    "    e_pars = CU.edit_params(dat.SquareEntropy.Processed.outputs.entropy_fit.params, 'const', 0, True)\n",
    "    efit = E.entropy_fits(x, e_signal, params=e_pars)[0]\n",
    "    efit_info = DA.FitInfo()\n",
    "    efit_info.init_from_fit(efit)\n",
    "    \n",
    "    td = dat.SquareEntropy.Processed.outputs.averaged\n",
    "    ed = [efit_info.eval_fit(x=x), e_signal]\n",
    "    intd = integrated_data\n",
    "    if filter_on is True:\n",
    "        scan_freq = dat.Logs.Fastdac.measure_freq/dat.AWG.info.wave_len\n",
    "        num_steps = dat.AWG.info.num_steps\n",
    "        decimate_factor = int(np.floor(num_steps/200))\n",
    "        if decimate_factor >= 2:\n",
    "            td = CU.decimate(td, scan_freq, decimate_factor=decimate_factor)\n",
    "            x = np.linspace(x[0], x[-1], td.shape[-1])\n",
    "            ed = [efit_info.eval_fit(x=x), CU.decimate(ed[1], scan_freq, decimate_factor=decimate_factor)]\n",
    "            intd = CU.decimate(intd, scan_freq, decimate_factor=decimate_factor)\n",
    "        else:\n",
    "            pass  # Decimate factor must be 2 or more\n",
    "    else:\n",
    "        pass\n",
    "    xs.append(x)\n",
    "    trans_datas.append(td)\n",
    "    entropy_datas.append(ed)\n",
    "    integrated_datas.append(intd)\n",
    "    \n",
    "    channel_ratio = round(dat.AWG.AWs[1][0][1]/dat.AWG.AWs[0][0][1]/-0.028, 2)\n",
    "    dsp = dat.SquareEntropy.Processed\n",
    "    ii = dsp.outputs.integrated_info\n",
    "    fit_text = f'SF={ii.sf:.2f}, Amp={ii.amp:.3f}nA, T_cold={tc:.3f}mV, T_hot={th:.3f}mV, dT={dT:.3f}mV, fit_dS={efit_info.best_values.dS:.3f}kB'\n",
    "    scan_text = f'HQPCbiases=({dat.AWG.AWs[0][0][1]:.0f}mV, {dat.AWG.AWs[0][0][3]:.0f}mV), Channel Biases=({dat.AWG.AWs[1][0][1]:.1f}mV, {dat.AWG.AWs[1][0][3]:.1f}mV), Perp Field={dat.Other.magy.field:.1f}mT'\n",
    "    array_text = f'Channel Bias Ratio = {channel_ratio}, Square Wave Frequency = {scan_freq:.1f}Hz, Sweeprate (LP*200) = {dat.Logs.sweeprate:.1f}mV/s'\n",
    "    text_info = f'{array_text}<br>{scan_text}<br>{fit_text}'\n",
    "    trans_titles.append(f'Dat{dat.datnum}: Averaged CS data<br>{text_info}')\n",
    "    entropy_titles.append(f'Dat{dat.datnum}: Entropy data with fit<br>{text_info}')\n",
    "    integrated_titles.append(f'Dat{dat.datnum}: Integrated Entropy data<br>{text_info}')\n",
    "    \n",
    "    \n",
    "fig1 = PlU.get_figure(datas=trans_datas, xs=xs, ids=ids, titles=trans_titles, labels=['v0_0', 'vp', 'v0_1', 'vm'], xlabel=f'{dat.Logs.x_label}', ylabel='Current /nA', plot_kwargs={'mode':'lines+markers'})\n",
    "fig2 = PlU.get_figure(datas=entropy_datas, xs=xs, ids=ids, titles=entropy_titles, labels=['fit', 'data'], xlabel=f'{dat.Logs.x_label}', ylabel='Current /nA', plot_kwargs={'mode':'lines+markers'})\n",
    "fig3 = PlU.get_figure(datas=integrated_datas, xs=xs, ids=ids, titles=integrated_titles, xlabel=f'{dat.Logs.x_label}', ylabel='Entropy /kB', plot_kwargs={'mode':'lines+markers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graphs=save_graphs\n",
    "for fig, name in zip([fig1, fig2, fig3], ['AveragedCS_vs_Channel_bias', 'Average_Entropy_With_Fit', 'Averaged_Integrated_Entropy']):\n",
    "    fig.update_layout(hovermode = 'x unified',\n",
    "                     title=dict(y=0.95,x=0.5,xanchor='center',yanchor='top', font=dict(size=14)))\n",
    "    if save_graphs: \n",
    "        fig.write_html(export_path+f'Dats{dats[0].datnum}-{dats[-1].datnum}{name}.html')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fit_entropies = list()\n",
    "for dat in dats:\n",
    "    x = dat.SquareEntropy.Processed.outputs.x\n",
    "    e_signal = dat.SquareEntropy.Processed.outputs.entropy_signal\n",
    "    e_pars = CU.edit_params(dat.SquareEntropy.Processed.outputs.entropy_fit.params, 'const', 0, True)\n",
    "    efit = E.entropy_fits(x, e_signal, params=e_pars)[0]\n",
    "    efit_info = DA.FitInfo()\n",
    "    efit_info.init_from_fit(efit)\n",
    "    fit_entropies.append(efit_info.best_values.dS)\n",
    "\n",
    "integrated_entropies = [dat.SquareEntropy.Processed.outputs.integrated_info.dS for dat in dats]\n",
    "lcts = [dat.Logs.fds['LCT'] for dat in dats]\n",
    "sfs = [dat.SquareEntropy.Processed.outputs.integrated_info.sf for dat in dats]\n",
    "int_texts = [f'{dat.datnum}<br>{sf:.3f}' for dat, sf in zip(dats, sfs)]\n",
    "fit_texts = [f'{dat.datnum}' for dat in dats]\n",
    "\n",
    "\n",
    "fit_trace = go.Scatter(x=lcts, y=fit_entropies, text=fit_texts, name='Fit Entropy', mode='lines+markers+text')\n",
    "\n",
    "integrated_trace = go.Scatter(x=lcts, y=integrated_entropies, text=int_texts, mode='lines+markers+text', name='Integrated Entropy')\n",
    "fig.add_traces([fit_trace, integrated_trace])\n",
    "fig.update_layout(xaxis_title='LCT /mV',\n",
    "                 yaxis_title='Entropy /kB',\n",
    "                 title=f\"Dats{dats[0].datnum}-{dats[-1].datnum}: Integrated entropy vs LCT with scaling factor shown as text\")\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.to_dict()['layout']['xaxis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = get_dats(range(579, 588+1))  # Weakly coupled dats only\n",
    "row = 0\n",
    "xs, datas, amps, thetas, gs = list(), list(), list(), list(), list()\n",
    "for dat in dats:\n",
    "    data, f = CU.decimate(dat.Transition.data[row], dat.Logs.Fastdac.measure_freq, numpnts=1000, return_freq=True)\n",
    "    print(f'Decimating with frequecy = {f}Hz')\n",
    "    x = CU.get_matching_x(dat.Transition.x, data)  # Makes x array with correct shape\n",
    "    fit = dat.Transition.all_fits[row]\n",
    "    amp = fit.best_values.amp\n",
    "    theta = fit.best_values.theta\n",
    "    if 'g' in fit.best_values.keys:\n",
    "        g = fit.best_values.g\n",
    "        gs.append(g)\n",
    "    xs.append(x)\n",
    "    datas.append(data)\n",
    "    amps.append(amp)\n",
    "    thetas.append(theta)\n",
    "    \n",
    "\n",
    "raw_fig = PlU.get_figure(datas, xs)\n",
    "avg_fig\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDatKernel",
   "language": "python",
   "name": "pydatkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
